
DOWNLOADING AND PRE-PROCESSING UNLIMITED OLD/BACKDATED TWITTER BIG DATA FOR ANALYSIS 


Data is the new oil, it is everywhere. Over the last decade more than 90 percent of Big Data has been generated by people living in urban areas. With the advent of internet of things and the increased use of the internet, social media has become an integral part of people’s daily lives. Millions of unstructured data are being sent to the cloud every second providing free and unbiased information practically on any discourse. Every second, an average of around 6000 tweets are tweeted on Twitter which corresponds to over 350,000 tweets sent per minute, 500 million tweets per day and around 200 billion tweets per year. See stats here.
I was working on a project recently centered around Community Resilience and Studentification, and I needed to download and analyze Old/Backdated Twitter data from about 10 years, naively I went about the very standard approach which was first creating a developer account with twitter, and then querying Twitter’s RESTful API to download the user tweets with some other meta-data such as retweets, likes, favorites, date and time, username, et cetera.
I was soon to discover that going down that road would not be very helpful to my task which was to download twitter data on a specific discourse over the last 10 years, which would then be fed into several pipelines I had built for analysis. And that was because of Twitter’s rate limits and time constraints; using twitter’s API you can not mine tweets older than seven days, similarly for a single search query, twitter only gives 100 results of the current week. Not very helpful to me I must say.
not very helpul. :-)
So I went into my default research mode. I had a problem to be solved which was to get old tweets programatically, then I found out about the GetOldTweets tool originally built by Jefferson Henrique on Github. I looked at it and saw that it was essentially a web scraping script that scrapes twitter search via the browser using key tools such as PyQuery, Lxml, and Beautifulsoup4. Interesting! and very good for my task. 
And how the algorithm works is that it scrapes twitter search very fast, looking for the search keywords you specified in your code, until the end of your search duration. And if you didn’t specify any time frame, it would scrape twitter-search until it gets to the very last tweet it can find on that keyword.
I also saw in the readme that the package assumes Python2.x only and the Python3.x implementation was still largely experimental. I immediately knew I had some debugging to do because I am not really a fan of Python2 as a matter of fact, its entering its retirement soon. Plus since the GetOldTweets package by Jefferson Henrique is a tool I would be using recurrently, why not just look at the Python3 implementation.
Ofcourse I did. And in that process I found another tool created by Dmitry Mottl, which was essentially an improvement fork of the original GetOldTweets package written by Jefferson Henrique. It had just what I was looking for; The python3 implementation. Essentially in software development and Engineering, you don’t try to recreate the wheels. So I jumped on it, and found it difficult to implement for my own use case. I wasn’t getting the results I needed even with the command line implementation. And that led to my own improvement fork. Which is the crux of this article.
So we are just getting started. 
TL;DR


My improvement fork of the code ensures that downloading millions of backdated/old tweets becomes relatively easier and stressless either on your Windows, Ubuntu, or Mac OS powered machine from command line. Just by following the simple steps which I have priorly enumerated on my Github but would show practically here. 
The first prerequisite would be to have any python3.x version installed on your local machine, and you should have already set the environment variable path, so you can interactively fire up python from your command or terminal without getting any error. The easiest way to do that on Windows is to run the python installer again and tick the box saying “Add Python to environment variables” under the advanced option. This answer on stackoverflow details my thoughts on that. And on terminal, if you’re using the Ubuntu distribution which I think is more popular among programmers that prefer the Linux OS, just run sudo apt-get install python3.6 on your terminal and you should be able to fire up python3 from terminal after installation by just typing “python”
After doing that, the next major requirement would be to install pyQuery and Lxml for handling requests and xml/html document types. This could easily be done by just running on terminal or command prompt; pip install pyquery and pip install lxml

Now once you’ve completed all these prerequisites, Head down to my Github here, to fork and/or clone the improvement fork of Mottl,’s python3 version which is also an improvement fork of the original package written by Jefferson Henrique. There’s a readme which contains the implementations and some basic examples attached to my repo, but i’ll be giving a short tutorial on downloading old/backdated twitter data.

Now once you have cloned or downloaded the repo to your local machine, you can start getting your old/backdated twitter data for any kind of analysis by:
1. cd into the downloaded GetOldTweets3 folder on your local machine and fire up cmd right in that same folder.
2. run the commands from the github readme to start downloading the twitter data which would then be saved on your computer as an output.csv file. But you can specify a name to save your downloaded dataset by passing a “name”.csv to the “--output” argument. For instance: --output dataset.csv
Now, getting our hands dirtier with more examples. Lets say we need to download tweets by a specific User.
We simply do step 1, as stated above, and type in the following command culled from the Github repo
python GetOldTweets3.py --username "mo4president"  --maxtweets 50 --output dataset.csv
this would scrape the twitter search engine and download all of the tweets made by @mo4president, you can also specify --maxtweets to control the amount of tweets you need, and --output to specify the output name. Leaving out the --maxtweets argument basically downloads all of the tweets by the user.
Also if you need to download tweets with respect to a particular keyword e.g “rent” over a period of time from like say 2014 till date and within a geographic geolocation, which is about 5 years worth of old tweets and could accumulate to millions of tweets data depending on your search radius and strings. Something similar to my task, you use this command also culled from my github repo, after executing step 1 stated above.
python GetOldTweets3.py --querysearch "rent" --near "6.52, 3.37" --within 40km --maxtweets 350 --output rent.csv --since 2014-01-01 --until 2019-08-31
So this command above would fetch you all the tweets that contains the word “rent”, within a 40km radius of the latitudinal and longitudinal coordinates of Lagos state from 2014 till date, and would store the results in a rent.csv file that would be saved in that same directory you’re working on. The maxtweets here would return 350 tweets because that is what was specified.

Running this same process I was able to gather enough big data on several discourse represented by keywords and analysed them accordingly to see trends relating to community resilience and studentification amongst other analysis over a 10 year period.
As shown above, after downloading the twitter big data in its raw unstructured state, you’ll need to do a bit of dataframe manipulations with pandas library in python to drop some columns and probably do some basic exploratory data analysis, you’ll also need some regular expressions to clean the tweets, sieve out unwanted hashtags and symbols that literally have no meaning; that could be done using python’s regular expressions module. Then you might want to pass the tweet texts into a sentiment analysis pipeline built around vader or textblob for further analysis or do a document-topic-word modeling using Gemsim’s library in python and the Latent Dirichlet Allocation model. But all these different analysis depends on your proficiency as a Data Scientist or Researcher.
In the next couple of posts i’ll be looking at pretty much all the analysis that could be carried out with the tweet texts to derive better insights, and also how to handle noise in your dataset. But for now, I hope you find this article helpful in gathering your twitter “big” data. 
